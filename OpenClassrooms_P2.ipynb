{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/o0nekov0o/OpenClassrooms_P2/blob/main/OpenClassrooms_P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJwIIB0s4L_s"
      },
      "outputs": [],
      "source": [
        "!pip install virtualenv\n",
        "\n",
        "!virtualenv OCP2_env\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip list\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip install requests\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip install bs4\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip install selenium\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip install webdriver-manager\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip list\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip freeze > OCP2_requirements.txt\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip install -r OCP2_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgONQxyPKHl4"
      },
      "outputs": [],
      "source": [
        "!pip install -r OCP2_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-0O7M-QOS6H"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "\n",
        "EXAMPLE_URL = 'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html' # global var\n",
        "def get_book_data(EXAMPLE_URL):\n",
        "    response = requests.get(EXAMPLE_URL)  # 200 si ok\n",
        "    print(response)  # affichage code retour\n",
        "    if response.ok:  # si justement ok\n",
        "        soup = BeautifulSoup(response.text, features='html.parser')  # format txt, delete warnings\n",
        "        \"\"\"driver = webdriver.Chrome(ChromeDriverManager().install())\n",
        "        driver.get(EXAMPLE_URL)\n",
        "        product_page_url = driver.current_url\"\"\"\n",
        "        product_page_url = EXAMPLE_URL #soup.find pour nom classe exacte, soup.select nom classe inexaxte\n",
        "        universal_product_code = soup.find('th', text='UPC').find_next_sibling('td').text  # colonne suivante, conversion txt\n",
        "        title = soup.select('div.col-sm-6.product_main > h1')[0].text # unique indice, conversion txt\n",
        "        price_excluding_tax = soup.find('th', text='Price (excl. tax)').find_next_sibling('td').text.replace('Â', '')  # comme title, enleve specialchar\n",
        "        price_including_tax = soup.find('th', text='Price (incl. tax)').find_next_sibling('td').text.replace('Â', '')  # comme title, enleve specialchar\n",
        "        number_available = soup.find('th', text='Availability').find_next_sibling('td').text  # comme title, conversion txt\n",
        "        product_description = soup.find('div', {'id': 'product_description'}).find_next_sibling('p').text  # comme title, balises differentes\n",
        "        category = soup.find('li', {'class': 'active'}).find_previous_sibling('li').find('a').text  # inverse title, balises differentes, recuperation lien\n",
        "        review_rating = f\"{soup.select('p.star-rating')[0]['class'][1]} Star(s) on Five\" # print interpreter, comportement code, liste dans liste\n",
        "        image_url = soup.select('div.item.active > img')[0]['src'].replace('../..', 'http://books.toscrape.com') # comme review_rating, liste attribut src\n",
        "        print(f\"Product Page URL : {product_page_url}\") # fprint on evite, + et \\n, code plus lisible\n",
        "        print(f\"UPC: {universal_product_code}\")\n",
        "        print(f\"Title: {title}\")\n",
        "        print(f\"Price Excluding Tax: {price_excluding_tax}\")\n",
        "        print(f\"Price Including Tax: {price_including_tax}\")\n",
        "        print(f\"Number Available: {number_available}\")\n",
        "        print(f\"product_description: {product_description}\")\n",
        "        print(f\"category: {category}\")\n",
        "        print(f\"Review Rating: {review_rating}\")\n",
        "        print(f\"Image URL: {image_url}\")\n",
        "        print('―' * 100) # ligne separation\n",
        "        save_image_file(image_url) # appel fonction, sauvegarde image\n",
        "        return {'Product Page URL': product_page_url, 'UPC': universal_product_code, 'Title': title, # strings headers, vars contents\n",
        "                'Price Excluding Tax': price_excluding_tax, 'Price Including Tax': price_including_tax, # pour ecrire contenu sous headers correspondants\n",
        "                'Number Available': number_available, 'Product Description': product_description,\n",
        "                'Category': category, 'Review Rating': review_rating, 'Image URL': image_url}\n",
        "\n",
        "def save_book_csv(book_data):\n",
        "    file_exists = os.path.isfile(f\"scraping_{list(book_data.values())[7].replace(' ','_')}.csv\") # permettra verifier si csv existe\n",
        "    file = open(f\"scraping_{list(book_data.values())[7].replace(' ','_')}.csv\", 'a', encoding='utf-8') # retour get_book_data, value index 7, cat name\n",
        "    headers = list(book_data) # retourne clés book_data dans liste pour headers\n",
        "    writer = csv.DictWriter(file, fieldnames=headers) #preparation ecriture headers\n",
        "    if not file_exists:\n",
        "        writer.writeheader() # si fichier n'existe pas, ecrire headers creation\n",
        "    writer.writerow(book_data) # ecrire contenu sous headers correspondants\n",
        "    file.close()\n",
        "\n",
        "index_url = 'http://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html' # global var\n",
        "def save_one_category(index_url):\n",
        "    \"\"\"\n",
        "    Save all books of a category (it handles pagination)\n",
        "    \"\"\"\n",
        "    suivant = 1 # permettra de relancer la boucle\n",
        "    url = index_url # preserve global var\n",
        "    while suivant == 1: #condition relance boucle\n",
        "        response = requests.get(url)  # 200 si ok\n",
        "        print(response)  # afficher code retour\n",
        "        if response.ok:  # si justement ok\n",
        "            soup = BeautifulSoup(response.text, features='html.parser') # format txt, delete warnings\n",
        "            for i in soup.find_all('div', {'class': 'image_container'}): # plusieurs classes image_container\n",
        "                lien = i.find('a')['href'].replace('../../..','http://books.toscrape.com/catalogue') # recup lien\n",
        "                book_data = get_book_data(lien) # appel fonction get_book_data, retour dans book_data           \n",
        "                save_book_csv(book_data) # appel foncion save_book_csv, enregistrer valeurs retournées\n",
        "                time.sleep(0.25) # meilleure lecture dans interpreter, si corrections bugs\n",
        "            if soup.find('li', {'class': 'next'}): # si presence bouton suivant\n",
        "                base_url = url[:url.rfind('/')] # sup derniere partie url (initialement index)\n",
        "                suffix_url = soup.find('li', {'class': 'next'}).find('a')['href'] # isole lien bouton suivant\n",
        "                url = f\"{base_url}/{suffix_url}\" # ajout lien bouton suivant au reste\n",
        "                # url = f\"{url[:url.rfind('/')]}/{soup.find('li', {'class': 'next'}).find('a')['href']}\"\n",
        "                suivant = 1 # relancer la boucle (car bouton suivant present)\n",
        "            else:\n",
        "                suivant = 0 # sortir de la boucle (car bouton suivant absent)\n",
        "    \n",
        "site_url = 'http://books.toscrape.com/index.html' # global var\n",
        "def save_all_categories(site_url):\n",
        "    curl = site_url # preserve global var\n",
        "    response = requests.get(curl)  # 200 si ok\n",
        "    print(response)  # afficher code retour\n",
        "    if response.ok:  # si justement ok\n",
        "        soup = BeautifulSoup(response.text, features='html.parser') # format txt, delete warnings\n",
        "        for x in soup.find_all('ul', {'class': 'nav nav-list'}): # unique classe 'nav nav-list' mais find_all iteration\n",
        "            links = x.find('ul').find_all('a') # autre ul, lister tous les lien des iterations\n",
        "        for n in range(len(links)): # parcourir toute la liste de liens récupérés\n",
        "            link = links[n]['href'].replace('catalogue', 'http://books.toscrape.com/catalogue') # affecter dans variable\n",
        "            try:\n",
        "                save_one_category(link) # même variable pour appeler fonction\n",
        "            except AttributeError: # si description absente\n",
        "                continue\n",
        "            time.sleep(0.25)\n",
        "\n",
        "class save_image_global: # pour créer fonction global\n",
        "    global save_image_file # car appelé avant par get_book_data\n",
        "    def save_image_file(url_image):\n",
        "        if os.path.isdir('scraping_images') == False: # si le dossier n'existe pas \n",
        "            os.makedirs('scraping_images') # alors procéder à sa création\n",
        "        img_url = url_image.replace('http://books.toscrape.com/media/cache/','').replace('/','_') # preparer nom image\n",
        "        img_data = requests.get(url_image).content # preparer contenu image\n",
        "        with open(f'scraping_images/{img_url}', 'wb') as handler: # from prepared image name\n",
        "            handler.write(img_data) # ecrire contenu image (dessiner l'img)\n",
        "\n",
        "save_all_categories(site_url)\n",
        "\n",
        "if __name__ == \"main\":\n",
        "    book_data = get_book_data(EXAMPLE_URL)\n",
        "    save_book_csv(book_data)\n",
        "    save_one_category(index_url)\n",
        "    save_all_categories(site_url)\n",
        "    save_image_file(url_image)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import glob\n",
        "\n",
        "reponse = 'o'\n",
        "while not reponse == 'y' or 'n':\n",
        "    reponse = input('Reset the repertory, yes (y) or no (n) ? ')\n",
        "    if reponse == 'y':\n",
        "        shutil.rmtree('/content/scraping_images')\n",
        "        for filename in glob.glob('/content/scraping_*'):\n",
        "            os.remove(filename)\n",
        "    elif reponse == 'n':\n",
        "        break"
      ],
      "metadata": {
        "id": "wM3r4mxbukii"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "OpenClassrooms_P2",
      "provenance": [],
      "authorship_tag": "ABX9TyPPyo10zCePjkm+zzQJwiQy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}