{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/o0nekov0o/OpenClassrooms_P2/blob/main/OpenClassrooms_P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Le nommage des livrables à déposer sur la plateforme a été changé et des indications sur les temps de soutenance ont été ajoutées - 20/04/2022.`\n",
        "\n",
        "# **Scénario**\n",
        "\n",
        "Vous êtes analyste marketing chez Books Online, une importante librairie en ligne spécialisée dans les livres d'occasion. Dans le cadre de vos fonctions, vous essayez de suivre manuellement les prix des livres d'occasion sur les sites web de vos concurrents, mais cela représente trop de travail et vous n'arrivez pas à y faire face  : il y a trop de livres et trop de librairies en ligne  ! Vous et votre équipe avez décidé d'automatiser cette tâche laborieuse via un programme (un scraper) développé en Python, capable d'extraire les informations tarifaires d'autres librairies en ligne. \n",
        " \n",
        "Sam, votre responsable d'équipe, vous a chargé de développer une version bêta de ce système pour suivre les prix des livres chez [Books to Scrape](http://books.toscrape.com/), un revendeur de livres en ligne. En pratique, dans cette version bêta, votre programme n'effectuera pas une véritable surveillance en temps réel des prix sur la durée. Il s'agira simplement d'une application exécutable à la demande visant à récupérer les prix au moment de son exécution.\n",
        "\n",
        "Sam vous a envoyé l'e-mail suivant : \n",
        "\n",
        "```\n",
        "Objet : Programme d'extraction des prix\n",
        "\n",
        "À : Vous\n",
        "\n",
        "De : Sam\n",
        "```\n",
        "\n",
        "Bonjour ! \n",
        "\n",
        "J'espère que vous pourrez m'aider à créer un système de surveillance des prix. Pour élaborer une version bêta du système limitée à un seul revendeur, le mieux est probablement de suivre les étapes que j'ai définies ci-dessous.\n",
        "\n",
        "Choisissez n'importe quelle page Produit sur le site de Books to Scrape. Écrivez un script Python qui visite cette page et en extrait les informations suivantes :\n",
        "\n",
        "* product_page_url\n",
        "* universal_product_code (upc)\n",
        "* title\n",
        "* price_including_tax\n",
        "* price_excluding_tax\n",
        "* product_description\n",
        "* category\n",
        "* review_rating\n",
        "* image_url\n",
        "\n",
        "Écrivez les données dans un fichier CSV qui utilise les champs ci-dessus comme en-têtes de colonnes.\n",
        "\n",
        "Maintenant que vous avez obtenu les informations concernant un premier livre, vous pouvez essayer de récupérer toutes les données nécessaires pour toute une catégorie d'ouvrages. Choisissez n'importe quelle catégorie sur le site de Books to Scrape. Écrivez un script Python qui consulte la page de la catégorie choisie, et extrait l'URL de la page Produit de chaque livre appartenant à cette catégorie. Combinez cela avec le travail que vous avez déjà effectué afin d'extraire les données produit de tous les livres de la catégorie choisie, puis écrivez les données dans un seul fichier CSV.\n",
        "\n",
        "Remarque : certaines pages de catégorie comptent plus de 20 livres, qui sont donc répartis sur différentes pages («  pagination  »). Votre application doit être capable de parcourir automatiquement les multiples pages si présentes. \n",
        "\n",
        "Ensuite, étendez votre travail à l'écriture d'un script qui consulte le site de [Books to Scrape](http://books.toscrape.com/), extrait toutes les catégories de livres disponibles, puis extrait les informations produit de tous les livres appartenant à toutes les différentes catégories, ce serait fantastique  ! Vous devrez écrire les données dans un fichier CSV distinct pour chaque catégorie de livres.\n",
        "\n",
        "Enfin, prolongez votre travail existant pour télécharger et enregistrer le fichier image de chaque page Produit que vous consultez  !\n",
        "\n",
        "Au cours du projet, veillez à enregistrer votre code dans un repository GitHub et à effectuer des commits réguliers accompagnés de messages de commit clairs. N'oubliez pas que vous devez enregistrer un fichier requirements.txt sans enregistrer votre environnement virtuel dans le repository lui-même. Vous ne devez pas non plus y enregistrer vos fichiers CSV. Lorsque vous aurez terminé, envoyez-moi un lien vers votre repository GitHub et un fichier compressé des données qu'il génère. Veillez également à prendre le temps d'écrire un fichier README.md, que vous ajouterez dans le repository afin que je puisse exécuter le code correctement et produire quelques données !\n",
        "\n",
        "Cordialement,\n",
        "\n",
        "Sam\n",
        "Responsable d'équipe\n",
        "\n",
        "Books Online\n",
        "\n",
        "Ces besoins étant clarifiés, vous êtes prêt à mettre votre maîtrise de Python toute neuve au service de votre équipe !\n",
        "\n",
        "# **Livrables**\n",
        "\n",
        "* Un document TXT contenant le lien vers le **repository GitHub** qui doit contenir les éléments suivants:\n",
        "l'ensemble de votre code d'application ;\n",
        "le fichier requirements.txt, mais pas l'environnement virtuel lui-même ;\n",
        "un fichier README.md expliquant comment créer et activer l'environnement virtuel, puis exécuter le code d'application ;\n",
        "les données/images extraites ne doivent pas faire partie du repository lui-même.\n",
        "* Un fichier compressé ZIP contenant toutes les **data** : les **données extraites et les images associées** dans un format ou une structure facile à suivre.\n",
        "\n",
        "---\n",
        "\n",
        "Pour faciliter votre passage devant le jury, déposez sur la plateforme, dans un dossier zip nommé “***Titre_du_projet_nom_prénom***”, tous les livrables du projet comme suit : ***Nom_Prénom_n° du livrable_nom du livrable__date de démarrage du projet***. Cela donnera :  \n",
        "\n",
        "* *Nom_Prénom_1_repo_mmaaaa*\n",
        "* *Nom_Prénom_2_data_mmaaaa*\n",
        "\n",
        "Par exemple, le premier livrable peut être nommé comme suit : \n",
        "\n",
        "*Dupont_Jean_1_repo_012022*\n",
        "\n",
        "---\n",
        "\n",
        "# **Soutenance**\n",
        "\n",
        "Durant la présentation orale, l’évaluateur interprétera le rôle de Sam, votre responsable d'équipe. La soutenance est structurée de la manière suivante :\n",
        "\n",
        "**Présentation des livrables (15 minutes)** \n",
        "\n",
        "* Présentez l'application en décrivant le processus utilisé pour l'écrire et en mettant en évidence les sections extraction, transformation et chargement du code.\n",
        "* Faites à votre évaluateur une démonstration de l'application en l'exécutant « en direct ».\n",
        "* Présentez des idées d'améliorations futures du code.\n",
        "\n",
        "**Discussion (10 minutes)** \n",
        "\n",
        "* L’évaluateur jouera le rôle de votre responsable d'équipe. Il vous challengera sur votre méthode et vos livrables.\n",
        "\n",
        "**Debrief (5 minutes)**\n",
        "\n",
        "* À la fin de la soutenance, l'évaluateur arrêtera de jouer le rôle de Sam, votre responsable d'équipe pour vous permettre de débriefer ensemble.\n",
        "\n",
        "`Votre présentation devrait durer 15 minutes (+/- 5 minutes).  Puisque le respect des durées des présentations est important en milieu professionnel, les présentations en dessous de 10 minutes ou au-dessus de 20 minutes peuvent être refusées.`\n",
        "\n",
        "# **Compétence transversale**\n",
        "\n",
        "En plus des compétences spécialisées énumérées ci-dessous, concentrez-vous sur le développement et la démonstration de la **communication** en tant que compétence transversale (ou “soft skill”). Elle sera essentielle pour votre réussite dans ce projet et les suivants, ainsi que dans votre future carrière. N'hésitez pas à demander à votre mentor un retour au cours de la réalisation de votre projet.\n",
        "\n",
        "# **Compétences évaluées**\n",
        "\n",
        "* Appliquer les bases de la programmation en Python\n",
        "* Utiliser le contrôle de version avec Git et GitHub\n",
        "* Gérer les données à l'aide du processus ETL\n",
        "* Configurer un environnement Python"
      ],
      "metadata": {
        "id": "paSHNfYXPWc6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJwIIB0s4L_s"
      },
      "outputs": [],
      "source": [
        "!pip install virtualenv\n",
        "\n",
        "!virtualenv OCP2_env\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip list\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip install requests\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip install bs4\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip install selenium\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip install webdriver-manager\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip list\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip freeze > OCP2_requirements.txt\n",
        "\n",
        "!source /content/OCP2_env/bin/activate; pip install -r OCP2_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgONQxyPKHl4"
      },
      "outputs": [],
      "source": [
        "!pip install -r OCP2_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-0O7M-QOS6H"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "\n",
        "EXAMPLE_URL = 'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html' # global var\n",
        "def get_book_data(EXAMPLE_URL):\n",
        "    response = requests.get(EXAMPLE_URL)  # 200 si ok\n",
        "    print(response)  # affichage code retour\n",
        "    if response.ok:  # si justement ok\n",
        "        soup = BeautifulSoup(response.text, features='html.parser')  # format txt, delete warnings\n",
        "        \"\"\"driver = webdriver.Chrome(ChromeDriverManager().install())\n",
        "        driver.get(EXAMPLE_URL)\n",
        "        product_page_url = driver.current_url\"\"\"\n",
        "        product_page_url = EXAMPLE_URL #soup.find pour nom classe exacte, soup.select nom classe inexaxte\n",
        "        universal_product_code = soup.find('th', text='UPC').find_next_sibling('td').text  # colonne suivante, conversion txt\n",
        "        title = soup.select('div.col-sm-6.product_main > h1')[0].text # unique indice, conversion txt\n",
        "        price_excluding_tax = soup.find('th', text='Price (excl. tax)').find_next_sibling('td').text.replace('Â', '')  # comme title, enleve specialchar\n",
        "        price_including_tax = soup.find('th', text='Price (incl. tax)').find_next_sibling('td').text.replace('Â', '')  # comme title, enleve specialchar\n",
        "        number_available = soup.find('th', text='Availability').find_next_sibling('td').text  # comme title, conversion txt\n",
        "        product_description = soup.find('div', {'id': 'product_description'}).find_next_sibling('p').text  # comme title, balises differentes\n",
        "        category = soup.find('li', {'class': 'active'}).find_previous_sibling('li').find('a').text  # inverse title, balises differentes, recuperation lien\n",
        "        review_rating = f\"{soup.select('p.star-rating')[0]['class'][1]} Star(s) on Five\" # print interpreter, comportement code, liste dans liste\n",
        "        image_url = soup.select('div.item.active > img')[0]['src'].replace('../..', 'http://books.toscrape.com') # comme review_rating, liste attribut src\n",
        "        print(f\"Product Page URL : {product_page_url}\") # fprint on evite, + et \\n, code plus lisible\n",
        "        print(f\"UPC: {universal_product_code}\")\n",
        "        print(f\"Title: {title}\")\n",
        "        print(f\"Price Excluding Tax: {price_excluding_tax}\")\n",
        "        print(f\"Price Including Tax: {price_including_tax}\")\n",
        "        print(f\"Number Available: {number_available}\")\n",
        "        print(f\"product_description: {product_description}\")\n",
        "        print(f\"category: {category}\")\n",
        "        print(f\"Review Rating: {review_rating}\")\n",
        "        print(f\"Image URL: {image_url}\")\n",
        "        print('―' * 100) # ligne separation\n",
        "        save_image_file(image_url) # appel fonction, sauvegarde image\n",
        "        return {'Product Page URL': product_page_url, 'UPC': universal_product_code, 'Title': title, # strings headers, vars contents\n",
        "                'Price Excluding Tax': price_excluding_tax, 'Price Including Tax': price_including_tax, # pour ecrire contenu sous headers correspondants\n",
        "                'Number Available': number_available, 'Product Description': product_description,\n",
        "                'Category': category, 'Review Rating': review_rating, 'Image URL': image_url}\n",
        "\n",
        "def save_book_csv(book_data):\n",
        "    file_exists = os.path.isfile(f\"scraping_{list(book_data.values())[7].replace(' ','_')}.csv\") # permettra verifier si csv existe\n",
        "    file = open(f\"scraping_{list(book_data.values())[7].replace(' ','_')}.csv\", 'a', encoding='utf-8') # retour get_book_data, value index 7, cat name\n",
        "    headers = list(book_data) # retourne clés book_data dans liste pour headers\n",
        "    writer = csv.DictWriter(file, fieldnames=headers) #preparation ecriture headers\n",
        "    if not file_exists:\n",
        "        writer.writeheader() # si fichier n'existe pas, ecrire headers creation\n",
        "    writer.writerow(book_data) # ecrire contenu sous headers correspondants\n",
        "    file.close()\n",
        "\n",
        "index_url = 'http://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html' # global var\n",
        "def save_one_category(index_url):\n",
        "    \"\"\"\n",
        "    Save all books of a category (it handles pagination)\n",
        "    \"\"\"\n",
        "    suivant = 1 # permettra de relancer la boucle\n",
        "    url = index_url # preserve global var\n",
        "    while suivant == 1: #condition relance boucle\n",
        "        response = requests.get(url)  # 200 si ok\n",
        "        print(response)  # afficher code retour\n",
        "        if response.ok:  # si justement ok\n",
        "            soup = BeautifulSoup(response.text, features='html.parser') # format txt, delete warnings\n",
        "            for i in soup.find_all('div', {'class': 'image_container'}): # plusieurs classes image_container\n",
        "                lien = i.find('a')['href'].replace('../../..','http://books.toscrape.com/catalogue') # recup lien\n",
        "                book_data = get_book_data(lien) # appel fonction get_book_data, retour dans book_data           \n",
        "                save_book_csv(book_data) # appel foncion save_book_csv, enregistrer valeurs retournées\n",
        "                time.sleep(0.25) # meilleure lecture dans interpreter, si corrections bugs\n",
        "            if soup.find('li', {'class': 'next'}): # si presence bouton suivant\n",
        "                base_url = url[:url.rfind('/')] # sup derniere partie url (initialement index)\n",
        "                suffix_url = soup.find('li', {'class': 'next'}).find('a')['href'] # isole lien bouton suivant\n",
        "                url = f\"{base_url}/{suffix_url}\" # ajout lien bouton suivant au reste\n",
        "                # url = f\"{url[:url.rfind('/')]}/{soup.find('li', {'class': 'next'}).find('a')['href']}\"\n",
        "                suivant = 1 # relancer la boucle (car bouton suivant present)\n",
        "            else:\n",
        "                suivant = 0 # sortir de la boucle (car bouton suivant absent)\n",
        "    \n",
        "site_url = 'http://books.toscrape.com/index.html' # global var\n",
        "def save_all_categories(site_url):\n",
        "    curl = site_url # preserve global var\n",
        "    response = requests.get(curl)  # 200 si ok\n",
        "    print(response)  # afficher code retour\n",
        "    if response.ok:  # si justement ok\n",
        "        soup = BeautifulSoup(response.text, features='html.parser') # format txt, delete warnings\n",
        "        for x in soup.find_all('ul', {'class': 'nav nav-list'}): # unique classe 'nav nav-list' mais find_all iteration\n",
        "            links = x.find('ul').find_all('a') # autre ul, lister tous les lien des iterations\n",
        "        for n in range(len(links)): # parcourir toute la liste de liens récupérés\n",
        "            link = links[n]['href'].replace('catalogue', 'http://books.toscrape.com/catalogue') # affecter dans variable\n",
        "            try:\n",
        "                save_one_category(link) # même variable pour appeler fonction\n",
        "            except AttributeError: # si description absente\n",
        "                continue\n",
        "            time.sleep(0.25)\n",
        "\n",
        "class save_image_global: # pour créer fonction global\n",
        "    global save_image_file # car appelé avant par get_book_data\n",
        "    def save_image_file(url_image):\n",
        "        if os.path.isdir('scraping_images') == False: # si le dossier n'existe pas \n",
        "            os.makedirs('scraping_images') # alors procéder à sa création\n",
        "        img_url = url_image.replace('http://books.toscrape.com/media/cache/','').replace('/','_') # preparer nom image\n",
        "        img_data = requests.get(url_image).content # preparer contenu image\n",
        "        with open(f'scraping_images/{img_url}', 'wb') as handler: # from prepared image name\n",
        "            handler.write(img_data) # ecrire contenu image (dessiner l'img)\n",
        "\n",
        "save_all_categories(site_url)\n",
        "\n",
        "if __name__ == \"main\":\n",
        "    book_data = get_book_data(EXAMPLE_URL)\n",
        "    save_book_csv(book_data)\n",
        "    save_one_category(index_url)\n",
        "    save_all_categories(site_url)\n",
        "    save_image_file(url_image)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import glob\n",
        "\n",
        "reponse = 'o'\n",
        "while not reponse == 'y' or 'n':\n",
        "    reponse = input('Reset the repertory, yes (y) or no (n) ? ')\n",
        "    if reponse == 'y':\n",
        "        shutil.rmtree('/content/scraping_images')\n",
        "        for filename in glob.glob('/content/scraping_*'):\n",
        "            os.remove(filename)\n",
        "    elif reponse == 'n':\n",
        "        break"
      ],
      "metadata": {
        "id": "wM3r4mxbukii"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "OpenClassrooms_P2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}